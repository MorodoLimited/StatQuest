{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StatQuest Machine Learning Playlist Notes:\n",
    "\n",
    "https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-1: A Gentle Introduction to Machine Learning\n",
    "\n",
    "- Fitting the Training Data well but making poor predictions is called the __Bias-Varience Tradeoff__.\n",
    "\n",
    "- The idea about choosing the best machine learning method is __how well a method performs on testing data__. It can be any method performs well with your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-2: Cross Validation\n",
    "\n",
    "- Want to use the variables Chest Pain, Good Blood Circ, Blocked Arteries, Weight to predict if smo has heart disease. Using the data we have below, we can learn.\n",
    "\n",
    "| Chest Pain | Good Blood Circ | Blocked Arteries | Weight | __HeartDisease__ |\n",
    "| --- | --- | --- |--- | --- |\n",
    "| No | No | No | 125 | __No__ |\n",
    "| Yes | Yes | Yes | 180 | __Yes__ |\n",
    "| Yes | Yes | No | 210 | __No__ |\n",
    "| ... | ... | ... | ... | __...__ |\n",
    "\n",
    "- When a new patient comes we can use what we have learnt above. \n",
    "\n",
    "| Chest Pain | Good Blood Circ | Blocked Arteries | Weight | __HeartDisease__ |\n",
    "| --- | --- | --- |--- | --- |\n",
    "| No | Yes | No | 150 | __???__ |\n",
    "\n",
    "- __The important question is which machine learning method we should use to solve this problem__. Logistic regession, SVM, k-NN ...? __Cross Validation__ lets us compare different machine learning methods.\n",
    "\n",
    "#### Cross Vaidation:\n",
    "\n",
    "When you have a machine learning problem you have a dataset. The dataset is a collection of real observations. In order to decide your machine learning algorithms' parameters (__train__ your machine learning method) you have to use some part of your dataset. This part of the dataset is called the __training set__. The remaining part of the set is the __test set__ and using the test set you will decide how well the training set is set your parameters for your machine learning method to react to an unseen data (the test set).\n",
    "\n",
    "Now the question is how to seperate the data to training and testing sets. \n",
    "\n",
    "Imagine you have a dataset as shown below;\n",
    "\n",
    "<span style='background :yellow' > |-----|-----|-----|-----| </span>\n",
    "\n",
    "\n",
    "Now you want to train your method using 75% of the data and test it with 25% of it. The question is which 25% of the data you will use to test your data?\n",
    "\n",
    "|<span style='background :red' >----- </span>|-----|-----|-----|<br>\n",
    "|-----|<span style='background :red' >----- </span>|-----|-----|<br>\n",
    "|-----|-----|-----|<span style='background :red' >----- </span>|<br>\n",
    "\n",
    "Cross-Validation uses them all one at a time and summarizes it at the end. So use first 25% of data as test and the remaining as 75% of it as test and then repeat the procedure changing the parts of the data used. Then use the summary of all the data for machine learning method X. Then repeat it all for the machine learning method Y etc... Now you have a summary of how well the machine learning methods performed using the same dataset. Thus now you can decide which method you want to use for your problem. \n",
    "\n",
    "__Note that__ we have divided data into 4 block and this is called __4-fold cross validation__. Also note that this number (4) is arbitrary and can be anything. If you make each individual data a block (for the example above there will be 1 test and 19 traingn samples) it is called __Leave One Out Cross Validation__. In practice, dividing data into 10 blocks thus making it a __10-Fold Cross Validation__ is very common. Lastly cross validation can be used to guess a tuning parameter that is independent from our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-3: Confusion Matrix\n",
    "\n",
    "How to summarize how each machine learning method is peformed on test data. Create a __confusion matrix__ for each method to decide which machine learning method performed best.  \n",
    "\n",
    "The rows of confusion matrix: what is predicted\n",
    "The cols of confusion matrix: what is known \n",
    "\n",
    "| Confusion Matrix | Has Heart Disease | Does not have Heart Disease |\n",
    "| --- | --- | --- |\n",
    "| __Has Heart Disease__ | True Positives | False Positives |\n",
    "| __Does not have Heart Disease__ | False Negatives | True Negatives |\n",
    "\n",
    "Example: \n",
    "\n",
    "Apply Random Forest to testing data:\n",
    "\n",
    "| Confusion Matrix | Has Heart Disease | Does not have Heart Disease |\n",
    "| --- | --- | --- |\n",
    "| __Has Heart Disease__ | 142 | 22 |\n",
    "| __Does not have Heart__ | 29 | 110 |\n",
    "\n",
    "Apply K-Nearest neighbors to testing data:\n",
    "\n",
    "| Confusion Matrix | Has Heart Disease | Does not have Heart Disease |\n",
    "| --- | --- | --- |\n",
    "| __Has Heart Disease__ | 107 | 53 |\n",
    "| __Does not have Heart__ | 64 | 79 |\n",
    "\n",
    "The obvious choice is the Random Forest for this problem.\n",
    "\n",
    "Imagine another dataset:\n",
    "\n",
    "Given if people like Jurassic Park, Run for your Life, Movie X,  Movie Y, can you guess what is a persons favourite movie. In this example there are only 3 options, Troll 2, Gore Police, Cool As Ice\n",
    "\n",
    "| Jurassic Park | Run for your Life | Movie X |  Movie Y | __Favourite Movie__ |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Liked | Not Liked | Liked |  Not Liked | __Troll 2__ |\n",
    "| Not Liked | Liked | Not Liked | Liked | __Gore Police__ |\n",
    "| Not Liked | Not Liked | Liked | Not Liked | __Cool As Ice__ |\n",
    "\n",
    "Since you have 3 possible redictions you will have a 3x3 confusion matrix like below;\n",
    "\n",
    "| Confusion Matrix | Troll 2 | Gore Police | Cool As Ice |\n",
    "| --- | --- | --- | --- |\n",
    "| __Troll 2__ | <font color='green'>12</font> | <font color='red'>102</font> | <font color='red'>93</font> |\n",
    "| __Gore Police__ | <font color='red'>112</font> | <font color='green'>23</font> | <font color='red'>77</font> |\n",
    "| __Cool As Ice__ |  <font color='red'>83</font> | <font color='red'>94</font> | <font color='green'>17</font> |\n",
    "\n",
    "The diagonal of green numbers represent where the machine learning algorithm does the right thing.\n",
    "\n",
    "__The size of the confusion matrix is determined by the number of outputs we want to predict.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-4: Sensitivity and Specificity\n",
    "\n",
    "Given a confusion matrix, how to calcualte sensitivity and specificity?\n",
    "\n",
    "Example:\n",
    "\n",
    "| Confusion Matrix | Has Heart Disease | Does not have Heart Disease |\n",
    "| --- | --- | --- |\n",
    "| __Has Heart Disease__ | <font color='green'>139</font> | <font color='red'>20</font> |\n",
    "| __Does not have Heart__ | <font color='red'>32</font> | <font color='green'>112</font> |\n",
    "\n",
    "Sensitivity: \n",
    "\n",
    "What percentage of patients with heart disease are correctly identified.\n",
    "\n",
    "__Sensitivity = TP / TP + FN__\n",
    "\n",
    "__Sensitivity = 139 / 139 + 32 = 0.8128__\n",
    "\n",
    "Meaning: 81% of the patients __with heart disease__ are correctly identified with the chosen machine learning method.\n",
    "\n",
    "Specificity: \n",
    "\n",
    "What percentage of patients without heart disease are correctly identified.\n",
    "\n",
    "__Specificity = TN / TN + FP__\n",
    "\n",
    "__Specificity = 112 / 112 + 20 = 0.8484__\n",
    "\n",
    "Meaning: 85% of the patients __without heart disease__ are correctly identified with the chosen machine learning method.\n",
    "\n",
    "Now, machine learning method 1 has sensitivity of 81% and specificity of 85%. Imagine if another machine learning method has sensitivity of 83% and specificity of 83%, how to interpret this to make a decision? What this tells us is method 2 is better in identifiying positives while method 1 is better in identifiying negatives. Thus; \n",
    "\n",
    "Choose method 1: if identifiying negatives are more important\n",
    "\n",
    "Choose method 2: if identifiying positives are more important "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-5: Bias and Variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
